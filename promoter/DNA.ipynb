{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "(1, 165, 64)\n",
        "1 sequence\n",
        "\n",
        "165 time steps (one for each base)\n",
        "\n",
        "64-dimensional vector at each ste"
      ],
      "metadata": {
        "id": "y7wYKu5b4zQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Load & Preprocess Data\n",
        "# ----------------------------\n",
        "base_to_idx = {'A': 0, 'T': 1, 'C': 2, 'G': 3, 'N': 4}\n",
        "idx_to_base = {v: k for k, v in base_to_idx.items()}\n",
        "\n",
        "# Function to pad and tokenize\n",
        "def preprocess_sequence(seq, max_len=165):\n",
        "    seq = seq.upper()[:max_len]\n",
        "    seq += 'N' * (max_len - len(seq))\n",
        "    return [base_to_idx.get(base, 4) for base in seq]\n",
        "\n",
        "df1 = pd.read_csv('/content/ecoli_mpra_expr.csv')\n",
        "df2 = pd.read_csv('/content/ecoli_natural50bp_expr (1).csv')\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "combined_df['tokenized'] = combined_df['seq'].apply(preprocess_sequence)\n",
        "\n",
        "X_seq = np.array(combined_df['tokenized'].tolist())\n",
        "y_expr = combined_df['expr'].values.reshape(-1, 1)\n",
        "y_seq = np.expand_dims(X_seq, -1)  # needed for sparse_categorical_crossentropy\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Define Model\n",
        "# ----------------------------\n",
        "vocab_size = 5\n",
        "max_len = 165\n",
        "embedding_dim = 16\n",
        "latent_dim = 64\n",
        "\n",
        "input_seq = layers.Input(shape=(max_len,), dtype='int32')\n",
        "x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_seq)\n",
        "x = layers.Bidirectional(layers.GRU(64, return_sequences=False))(x)\n",
        "latent = layers.Dense(latent_dim, activation='relu', name=\"latent_vector\")(x)\n",
        "\n",
        "expr_pred = layers.Dense(1, name=\"expression_output\")(latent)\n",
        "\n",
        "x = layers.RepeatVector(max_len)(latent)\n",
        "x = layers.GRU(64, return_sequences=True)(x)\n",
        "decoded = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'), name=\"decoder_output\")(x)\n",
        "\n",
        "autoencoder = models.Model(inputs=input_seq, outputs=[decoded, expr_pred])\n",
        "autoencoder.compile(optimizer='adam',\n",
        "                    loss={'decoder_output': 'sparse_categorical_crossentropy', 'expression_output': 'mse'},\n",
        "                    loss_weights={'decoder_output': 1.0, 'expression_output': 1.0},\n",
        "                    metrics={'decoder_output': 'accuracy'})\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Train Model\n",
        "# ----------------------------\n",
        "autoencoder.fit(X_seq,\n",
        "                {'decoder_output': y_seq, 'expression_output': y_expr},\n",
        "                batch_size=64,\n",
        "                epochs=20,\n",
        "                validation_split=0.1)\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Create Generator Tools\n",
        "# ----------------------------\n",
        "latent_input = tf.keras.Input(shape=(latent_dim,))\n",
        "x = layers.RepeatVector(max_len)(latent_input)\n",
        "x = layers.GRU(64, return_sequences=True)(x)\n",
        "decoder_output = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))(x)\n",
        "decoder = tf.keras.Model(latent_input, decoder_output)\n",
        "\n",
        "expr_head = layers.Dense(1)(latent_input)\n",
        "expression_model = tf.keras.Model(latent_input, expr_head)\n",
        "\n",
        "def sample_with_temperature(probs, temperature=1.0):\n",
        "    probs = np.clip(probs, 1e-8, 1.0)\n",
        "    logits = np.log(probs) / temperature\n",
        "    exp_logits = np.exp(logits - np.max(logits))\n",
        "    probs = exp_logits / np.sum(exp_logits)\n",
        "    return np.random.choice(len(probs), p=probs)\n",
        "\n",
        "def decode_probs_to_seq(probs, temperature=1.0):\n",
        "    return ''.join([idx_to_base[sample_with_temperature(p, temperature)] for p in probs])\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Generate One Sequence for User Input\n",
        "# ----------------------------\n",
        "def generate_sequence_for_expression(target_expr, temperature=0.8, steps=500):\n",
        "    z = tf.Variable(tf.random.normal((1, latent_dim)), trainable=True)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
        "\n",
        "    for step in range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred = expression_model(z, training=False)\n",
        "            loss = tf.reduce_mean(tf.square(pred - target_expr))\n",
        "        grads = tape.gradient(loss, [z])\n",
        "        optimizer.apply_gradients(zip(grads, [z]))\n",
        "\n",
        "        if step % 100 == 0 or step == steps - 1:\n",
        "            print(f\"Step {step}: Predicted = {pred.numpy()[0][0]:.4f} | Target = {target_expr:.4f}\")\n",
        "\n",
        "    probs = decoder(z, training=False).numpy()[0]\n",
        "    decoded_seq = decode_probs_to_seq(probs, temperature)\n",
        "    final_pred = expression_model(z, training=False).numpy()[0][0]\n",
        "    return decoded_seq, final_pred\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Example Use\n",
        "# ----------------------------\n",
        "user_expr = float(input(\"Enter desired expression value (e.g., 0.8): \"))\n",
        "seq, pred = generate_sequence_for_expression(target_expr=user_expr)\n",
        "print(f\"\\n🧬 Generated Sequence: {seq}\\nPredicted Expression: {pred:.4f}\")"
      ],
      "metadata": {
        "id": "_X0kkou9Vsaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Step 1A: Define the mappings\n",
        "base_to_idx = {'A': 0, 'T': 1, 'C': 2, 'G': 3, 'N': 4}\n",
        "idx_to_base = {v: k for k, v in base_to_idx.items()}\n",
        "\n",
        "# Step 1B: Save them to a .pkl file\n",
        "with open('vocab_mapping.pkl', 'wb') as f:\n",
        "    pickle.dump({'base_to_idx': base_to_idx, 'idx_to_base': idx_to_base}, f)\n",
        "\n",
        "print(\"✅ Step 1 complete: Vocabulary mappings saved to vocab_mapping.pkl\")\n"
      ],
      "metadata": {
        "id": "m-t3rAq_pEgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the joint model (encoder-decoder + expression head)\n",
        "autoencoder.save(\"autoencoder_model.h5\")\n",
        "\n",
        "# Save the decoder (for generating sequence from z)\n",
        "decoder.save(\"decoder_model.h5\")\n",
        "\n",
        "# Save the expression prediction model (to estimate expression from z)\n",
        "expression_model.save(\"expression_model.h5\")\n",
        "\n",
        "print(\"✅ Step 2 complete: All models saved as .h5 files\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9C8K866jikM",
        "outputId": "2d444de1-a361-44b1-cebd-1bf9991670aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Step 2 complete: All models saved as .h5 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "# === Step 1: Load models ===\n",
        "autoencoder = tf.keras.models.load_model(\"autoencoder_model.h5\", compile=False)\n",
        "decoder = tf.keras.models.load_model(\"decoder_model.h5\", compile=False)\n",
        "expression_model = tf.keras.models.load_model(\"expression_model.h5\", compile=False)\n",
        "\n",
        "# === Step 2: Load vocabulary ===\n",
        "with open(\"vocab_mapping.pkl\", \"rb\") as f:\n",
        "    vocab_data = pickle.load(f)\n",
        "\n",
        "vocab = vocab_data[\"base_to_idx\"]\n",
        "idx_to_char = vocab_data[\"idx_to_base\"]\n",
        "\n",
        "# === Step 3: Decode one-hot to sequence ===\n",
        "def decode_sequence(one_hot_seq):\n",
        "    indices = np.argmax(one_hot_seq, axis=-1)\n",
        "    chars = [idx_to_char.get(i, 'N') for i in indices]\n",
        "    return ''.join(chars)\n",
        "\n",
        "# === Step 4: Decode with temperature ===\n",
        "def decode_sequence_with_temperature(logits, temperature=1.0):\n",
        "    probs = tf.nn.softmax(logits / temperature, axis=-1).numpy()\n",
        "    sampled_indices = [np.random.choice(len(p), p=p) for p in probs]\n",
        "    chars = [idx_to_char.get(i, 'N') for i in sampled_indices]\n",
        "    return ''.join(chars)\n",
        "\n",
        "# === Step 5: Clean DNA sequence (remove non-ACGT) ===\n",
        "def clean_dna_sequence(seq):\n",
        "    return re.sub(r'[^ACGT]', '', seq)\n",
        "\n",
        "# === Step 6: Latent optimization for expression ===\n",
        "latent_dim = autoencoder.get_layer(\"latent_vector\").output.shape[-1]\n",
        "\n",
        "def generate_sequence_for_expression(target_expr, steps=500, lr=0.05, temps=[0.4, 0.6, 0.8, 1.0]):\n",
        "    z = tf.Variable(tf.random.normal([1, latent_dim]), trainable=True)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    for step in range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred_expr = expression_model(z, training=False)\n",
        "            loss = tf.reduce_mean(tf.square(pred_expr - target_expr))\n",
        "        grads = tape.gradient(loss, [z])\n",
        "        optimizer.apply_gradients(zip(grads, [z]))\n",
        "\n",
        "        if step % 100 == 0 or step == steps - 1:\n",
        "            print(f\"Step {step}: Predicted = {pred_expr.numpy().squeeze():.4f} | Target = {target_expr:.4f}\")\n",
        "\n",
        "    logits = decoder(z, training=False).numpy().squeeze()\n",
        "\n",
        "    generated_variants = []\n",
        "    for temp in temps:\n",
        "        seq = decode_sequence_with_temperature(logits, temperature=temp)\n",
        "        pred_expr = expression_model(z, training=False).numpy().squeeze()\n",
        "        print(f\"\\n🌡️ Temp {temp:.1f} | Predicted: {pred_expr:.4f} | Sequence: {seq}\")\n",
        "        cleaned = clean_dna_sequence(seq)\n",
        "        if len(cleaned) == len(seq):  # Keep only valid ACGT sequences\n",
        "            generated_variants.append((temp, seq, pred_expr))\n",
        "\n",
        "    return generated_variants\n",
        "\n",
        "# === Step 7: Run and save results ===\n",
        "if __name__ == \"__main__\":\n",
        "    target_expression = float(input(\"Enter desired expression value (e.g., 5.0): \"))\n",
        "    results = generate_sequence_for_expression(target_expression)\n",
        "\n",
        "    output_file = f\"generated_sequences_expr_{target_expression:.2f}.txt\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for temp, seq, pred in results:\n",
        "            f.write(f\"> Temp: {temp:.1f}, Predicted: {pred:.4f}\\n{seq}\\n\\n\")\n",
        "\n",
        "    print(f\"\\n✅ All sequence variants saved to: {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMwv9r6Es-BN",
        "outputId": "9238f302-bd96-40e3-aa86-76ed30e96f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter desired expression value (e.g., 5.0): 5\n",
            "Step 0: Predicted = -0.9703 | Target = 5.0000\n",
            "Step 100: Predicted = 4.9934 | Target = 5.0000\n",
            "Step 200: Predicted = 5.0000 | Target = 5.0000\n",
            "Step 300: Predicted = 5.0000 | Target = 5.0000\n",
            "Step 400: Predicted = 5.0000 | Target = 5.0000\n",
            "Step 499: Predicted = 5.0000 | Target = 5.0000\n",
            "\n",
            "🌡️ Temp 0.4 | Predicted: 5.0000 | Sequence: TAAAAAAGAGTACGACNANACAANTTTANTANACNGATNAAAAGAAAAATAACNNCNTTAAAAAAGACATAGNAANACTGAACAACCGATAGACATGAGAATCAACTCNGNGAAACANACGAAAAAAAGAGANCANATNANACAAANANAAGCAAAACTAAAATA\n",
            "\n",
            "🌡️ Temp 0.6 | Predicted: 5.0000 | Sequence: GGAAAAAACNGNAACCANATATCNCCACTGCCCGATTAATACNAATGCNAATCCATAAATCAAAACGAGAAANCAGNTAGAAAAGGCGNATAAAAAGAAATGCAAAAAAAATAGGAANACGAGAACNNGTGGCCNTCTAGNAGCAGNGATAGTCNGCCTTAANTA\n",
            "\n",
            "🌡️ Temp 0.8 | Predicted: 5.0000 | Sequence: NACACTTGCCTAGNGAAAGACCGGTCGCGATTNNGTANCAAGGTAATTGANTNCAGCANCNNCAAGNCNCNTGACANTTTGATNGGNACATAGATACGATAANTAAAGTAAGGTTCTNTTAGAGNTNNGCTAGNATNTTNGATNAAGGCTNGCGNACATTAAGNT\n",
            "\n",
            "🌡️ Temp 1.0 | Predicted: 5.0000 | Sequence: GAGNCGGCCGAACTGNTCCACAATNAANAGTNACATGCGNCACTANATATAAAAGTNGAAAGTGCTTCCNTGAGGTCATAANACGCACTTAAGTCGCGNTNAAGAAAGATGACNCCTCACCATNAANGANAAAATTAAGNTNAGCCCAGTNAAATCNCAGTNGCC\n",
            "\n",
            "✅ All sequence variants saved to: generated_sequences_expr_5.00.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "krmQFL6VGcr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# import pickle\n",
        "# import re\n",
        "# import os\n",
        "\n",
        "# # --- Global Definitions (Assumes models and vocab files are available) ---\n",
        "# # It's generally good practice to pass models as arguments or ensure they are loaded once\n",
        "# # when the application starts, rather than repeatedly. For this example, we keep\n",
        "# # them loaded globally as per your original script's structure.\n",
        "\n",
        "# # Path to your model and vocabulary files\n",
        "# # Ensure these files are in the same directory as your script or provide full paths.\n",
        "# AUTOENCODER_MODEL_PATH = \"autoencoder_model.h5\"\n",
        "# DECODER_MODEL_PATH = \"decoder_model.h5\"\n",
        "# EXPRESSION_MODEL_PATH = \"expression_model.h5\"\n",
        "# VOCAB_MAPPING_PATH = \"vocab_mapping.pkl\"\n",
        "\n",
        "# # --- Function to load models ---\n",
        "# def load_models():\n",
        "#     \"\"\"Loads the pre-trained Keras models.\"\"\"\n",
        "#     try:\n",
        "#         autoencoder = tf.keras.models.load_model(AUTOENCODER_MODEL_PATH, compile=False)\n",
        "#         decoder = tf.keras.models.load_model(DECODER_MODEL_PATH, compile=False)\n",
        "#         expression_model = tf.keras.models.load_model(EXPRESSION_MODEL_PATH, compile=False)\n",
        "#         return autoencoder, decoder, expression_model\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading models: {e}\")\n",
        "#         print(\"Please ensure 'autoencoder_model.h5', 'decoder_model.h5', and 'expression_model.h5' are in the correct directory.\")\n",
        "#         return None, None, None\n",
        "\n",
        "# # --- Function to load vocabulary ---\n",
        "# def load_vocabulary():\n",
        "#     \"\"\"Loads the vocabulary mapping from a pickle file.\"\"\"\n",
        "#     try:\n",
        "#         with open(VOCAB_MAPPING_PATH, \"rb\") as f:\n",
        "#             vocab_data = pickle.load(f)\n",
        "#         vocab = vocab_data[\"base_to_idx\"]\n",
        "#         idx_to_char = vocab_data[\"idx_to_base\"]\n",
        "#         return vocab, idx_to_char\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading vocabulary: {e}\")\n",
        "#         print(\"Please ensure 'vocab_mapping.pkl' is in the correct directory.\")\n",
        "#         return None, None\n",
        "\n",
        "# # --- Helper Functions for Sequence Manipulation ---\n",
        "\n",
        "# def decode_sequence(one_hot_seq, idx_to_char_map):\n",
        "#     \"\"\"\n",
        "#     Decodes a one-hot encoded sequence back into a character string.\n",
        "\n",
        "#     Args:\n",
        "#         one_hot_seq (np.array): A NumPy array representing the one-hot encoded sequence.\n",
        "#         idx_to_char_map (dict): A dictionary mapping index to character.\n",
        "\n",
        "#     Returns:\n",
        "#         str: The decoded sequence string.\n",
        "#     \"\"\"\n",
        "#     indices = np.argmax(one_hot_seq, axis=-1)\n",
        "#     chars = [idx_to_char_map.get(i, 'N') for i in indices] # Use .get with default 'N' for safety\n",
        "#     return ''.join(chars)\n",
        "\n",
        "# def decode_sequence_with_temperature(logits, idx_to_char_map, temperature=1.0):\n",
        "#     \"\"\"\n",
        "#     Decodes a sequence from logits using a specified temperature for sampling.\n",
        "\n",
        "#     Args:\n",
        "#         logits (tf.Tensor or np.array): Raw prediction scores from the decoder.\n",
        "#         idx_to_char_map (dict): A dictionary mapping index to character.\n",
        "#         temperature (float): Controls the randomness of sampling. Higher values\n",
        "#                              (e.g., 1.0+) lead to more diverse sequences, lower values\n",
        "#                              (e.g., 0.1-0.5) lead to more conservative sequences.\n",
        "\n",
        "#     Returns:\n",
        "#         str: The sampled sequence string.\n",
        "#     \"\"\"\n",
        "#     # Ensure logits are a TensorFlow tensor for softmax\n",
        "#     if not isinstance(logits, tf.Tensor):\n",
        "#         logits = tf.convert_to_tensor(logits, dtype=tf.float32)\n",
        "\n",
        "#     # Apply temperature and softmax to get probabilities\n",
        "#     # Adding a small epsilon to temperature to prevent division by zero if temp is 0.\n",
        "#     probs = tf.nn.softmax(logits / (temperature + 1e-8), axis=-1).numpy()\n",
        "\n",
        "#     # Sample indices based on probabilities\n",
        "#     sampled_indices = [np.random.choice(len(p), p=p) for p in probs]\n",
        "\n",
        "#     # Convert indices back to characters\n",
        "#     chars = [idx_to_char_map.get(i, 'N') for i in sampled_indices]\n",
        "#     return ''.join(chars)\n",
        "\n",
        "# def clean_dna_sequence(seq):\n",
        "#     \"\"\"\n",
        "#     Cleans a DNA sequence by removing any characters that are not A, C, G, or T.\n",
        "\n",
        "#     Args:\n",
        "#         seq (str): The input sequence string.\n",
        "\n",
        "#     Returns:\n",
        "#         str: The cleaned sequence containing only ACGT characters.\n",
        "#     \"\"\"\n",
        "#     return re.sub(r'[^ACGT]', '', seq)\n",
        "\n",
        "# # --- Core Logic: Latent Optimization and Sequence Generation ---\n",
        "\n",
        "# def generate_sequence_for_expression(\n",
        "#     target_expr,\n",
        "#     autoencoder_model, # Passed as argument\n",
        "#     decoder_model,     # Passed as argument\n",
        "#     expression_model_ref, # Passed as argument (renamed to avoid conflict with imported name)\n",
        "#     idx_to_char_map,   # Passed as argument\n",
        "#     steps=500,\n",
        "#     lr=0.05,\n",
        "#     temps=[0.4, 0.6, 0.8, 1.0]\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Optimizes a latent vector to match a target expression value and\n",
        "#     then generates diverse DNA sequences using a decoder.\n",
        "\n",
        "#     Args:\n",
        "#         target_expr (float): The desired expression value to optimize for.\n",
        "#         autoencoder_model (tf.keras.Model): The loaded autoencoder model (used to get latent_dim).\n",
        "#         decoder_model (tf.keras.Model): The loaded decoder model.\n",
        "#         expression_model_ref (tf.keras.Model): The loaded expression prediction model.\n",
        "#         idx_to_char_map (dict): Dictionary mapping integer indices back to characters (e.g., 'A', 'C', 'G', 'T').\n",
        "#         steps (int): Number of optimization steps for the latent vector.\n",
        "#         lr (float): Learning rate for the Adam optimizer.\n",
        "#         temps (list): List of temperature values for diverse sequence generation.\n",
        "\n",
        "#     Returns:\n",
        "#         list: A list of tuples, where each tuple contains (temperature, generated_sequence, predicted_expression).\n",
        "#     \"\"\"\n",
        "#     # Get latent dimension from the autoencoder's latent layer\n",
        "#     # Assuming 'latent_vector' is the name of the latent layer in your autoencoder\n",
        "#     latent_dim = autoencoder_model.get_layer(\"latent_vector\").output.shape[-1]\n",
        "\n",
        "#     # Initialize the latent vector 'z' randomly\n",
        "#     z = tf.Variable(tf.random.normal([1, latent_dim]), trainable=True)\n",
        "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "#     print(f\"Starting latent optimization for target expression: {target_expr:.4f}\")\n",
        "#     for step in range(steps):\n",
        "#         with tf.GradientTape() as tape:\n",
        "#             # Predict expression for the current latent vector 'z'\n",
        "#             pred_expr = expression_model_ref(z, training=False)\n",
        "#             # Calculate mean squared error loss between predicted and target expression\n",
        "#             loss = tf.reduce_mean(tf.square(pred_expr - target_expr))\n",
        "\n",
        "#         # Compute gradients of the loss with respect to 'z'\n",
        "#         grads = tape.gradient(loss, [z])\n",
        "#         # Apply gradients to update 'z'\n",
        "#         optimizer.apply_gradients(zip(grads, [z]))\n",
        "\n",
        "#         # Print progress periodically\n",
        "#         if step % 100 == 0 or step == steps - 1:\n",
        "#             print(f\"Step {step}: Predicted = {pred_expr.numpy().squeeze():.4f} | Target = {target_expr:.4f} | Loss = {loss.numpy():.6f}\")\n",
        "\n",
        "#     print(\"\\nOptimization complete. Generating sequences...\")\n",
        "#     # Get logits from the decoder using the optimized latent vector 'z'\n",
        "#     logits = decoder_model(z, training=False).numpy().squeeze()\n",
        "\n",
        "#     generated_variants = []\n",
        "#     # Generate sequences at different temperatures\n",
        "#     for temp in temps:\n",
        "#         # Decode sequence using the specific temperature\n",
        "#         seq = decode_sequence_with_temperature(logits, idx_to_char_map, temperature=temp)\n",
        "#         # Re-predict expression for the optimized 'z' (this value will be consistent across temps)\n",
        "#         final_pred_expr = expression_model_ref(z, training=False).numpy().squeeze()\n",
        "\n",
        "#         print(f\"\\n🌡️ Temp {temp:.1f} | Predicted: {final_pred_expr:.4f} | Sequence: {seq}\")\n",
        "\n",
        "#         # Clean the generated sequence and validate if it's purely ACGT\n",
        "#         cleaned = clean_dna_sequence(seq)\n",
        "#         if len(cleaned) == len(seq):  # Keep only valid ACGT sequences\n",
        "#             generated_variants.append((temp, seq, final_pred_expr))\n",
        "#         else:\n",
        "#             print(f\"  Warning: Sequence contained non-ACGT characters and was not added.\")\n",
        "\n",
        "#     return generated_variants\n",
        "\n",
        "# # --- Main execution function ---\n",
        "\n",
        "# def run_dna_sequence_tool(target_expression_value: float, output_filename: str = None):\n",
        "#     \"\"\"\n",
        "#     Main function to run the DNA sequence generation tool.\n",
        "\n",
        "#     Args:\n",
        "#         target_expression_value (float): The desired expression value.\n",
        "#         output_filename (str, optional): The name of the file to save results.\n",
        "#                                          If None, a default name is generated.\n",
        "#     \"\"\"\n",
        "#     print(\"--- Initializing DNA Sequence Generation Tool ---\")\n",
        "\n",
        "#     # Load models\n",
        "#     autoencoder, decoder, expression_model_loaded = load_models()\n",
        "#     if autoencoder is None or decoder is None or expression_model_loaded is None:\n",
        "#         print(\"Exiting due to model loading error.\")\n",
        "#         return\n",
        "\n",
        "#     # Load vocabulary\n",
        "#     vocab, idx_to_char = load_vocabulary()\n",
        "#     if vocab is None or idx_to_char is None:\n",
        "#         print(\"Exiting due to vocabulary loading error.\")\n",
        "#         return\n",
        "\n",
        "#     # Generate sequences\n",
        "#     results = generate_sequence_for_expression(\n",
        "#         target_expr=target_expression_value,\n",
        "#         autoencoder_model=autoencoder,\n",
        "#         decoder_model=decoder,\n",
        "#         expression_model_ref=expression_model_loaded,\n",
        "#         idx_to_char_map=idx_to_char,\n",
        "#         steps=500, # Default steps\n",
        "#         lr=0.05,   # Default learning rate\n",
        "#         temps=[0.4, 0.6, 0.8, 1.0] # Default temperatures\n",
        "#     )\n",
        "\n",
        "#     # Determine output file name\n",
        "#     if output_filename is None:\n",
        "#         output_file = f\"generated_sequences_expr_{target_expression_value:.2f}.txt\"\n",
        "#     else:\n",
        "#         output_file = output_filename\n",
        "\n",
        "#     # Save results to file\n",
        "#     try:\n",
        "#         with open(output_file, \"w\") as f:\n",
        "#             for temp, seq, pred in results:\n",
        "#                 f.write(f\"> Temp: {temp:.1f}, Predicted: {pred:.4f}\\n{seq}\\n\\n\")\n",
        "#         print(f\"\\n✅ All valid sequence variants saved to: {os.path.abspath(output_file)}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error saving results to file {output_file}: {e}\")\n",
        "\n",
        "# # Example Usage (if you were to run this as a standalone script)\n",
        "# if __name__ == \"__main__\":\n",
        "#     # This block shows how you would typically call the main function.\n",
        "#     # In a real interactive environment, you might get this from user input or another part of your application.\n",
        "\n",
        "#     # Simulate user input for demonstration\n",
        "#     # You might replace this with a more robust input method in a web app, for example.\n",
        "#     try:\n",
        "#         desired_expression = float(input(\"Enter desired expression value (e.g., 5.0): \"))\n",
        "#         run_dna_sequence_tool(desired_expression)\n",
        "#     except ValueError:\n",
        "#         print(\"Invalid input. Please enter a numerical value for expression.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "UuV7H0KBGe15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde38a67-03ef-4f85-a84b-993dcd8f808b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter desired expression value (e.g., 5.0): 7.04\n",
            "--- Initializing DNA Sequence Generation Tool ---\n",
            "Starting latent optimization for target expression: 7.0400\n",
            "Step 0: Predicted = 1.0916 | Target = 7.0400 | Loss = 35.383472\n",
            "Step 100: Predicted = 7.0346 | Target = 7.0400 | Loss = 0.000029\n",
            "Step 200: Predicted = 7.0400 | Target = 7.0400 | Loss = 0.000000\n",
            "Step 300: Predicted = 7.0400 | Target = 7.0400 | Loss = 0.000000\n",
            "Step 400: Predicted = 7.0400 | Target = 7.0400 | Loss = 0.000000\n",
            "Step 499: Predicted = 7.0400 | Target = 7.0400 | Loss = 0.000000\n",
            "\n",
            "Optimization complete. Generating sequences...\n",
            "\n",
            "🌡️ Temp 0.4 | Predicted: 7.0400 | Sequence: GCTTNTNGGTTTGCNCGNGGATGGTGATTCNCNGGNNNTGACCTGTTCGGCGCCTTGAATCTCNNAATAGTCTGACNTGCNTTTNNCAGGAGATNCCAATTTAGNGGAATCTAAAACNTANNCCACTGATTCCATGGGTATNGNGGCNGNNCNTATCTTGGCTNT\n",
            "  Warning: Sequence contained non-ACGT characters and was not added.\n",
            "\n",
            "🌡️ Temp 0.6 | Predicted: 7.0400 | Sequence: GAGCAGATATTTNNGTNGGGANCCNTATNTCGAGATAATGTTTNCAGGCNGGATCGTTTATTNTNGAGAGGTGCANCGGTAANACAGTCGNGGTGCGCANGTAGCCTATCCGCACTTCAACGATNATCTGGNTCNTNACAGTGGGCATNNAATANNNGACCAGAC\n",
            "  Warning: Sequence contained non-ACGT characters and was not added.\n",
            "\n",
            "🌡️ Temp 0.8 | Predicted: 7.0400 | Sequence: ATAATTAANTAGTGGNTCANANGAGNCGGCCAAGACTGNACNCCTNTGCTNGGCNGACANGCCTGTATAGAATACNTGTTCATAANATNGATGTCGGGTANGTGAAGAGAAGNGTCNTTGTTTCGTNGTNATCGTGANTCNGTNCATGTACANGGNCCAAAGTTC\n",
            "  Warning: Sequence contained non-ACGT characters and was not added.\n",
            "\n",
            "🌡️ Temp 1.0 | Predicted: 7.0400 | Sequence: TNCAACATCCAAGATGTGNCGCGGTCGCANATCANNTCANTCNTNTGAATCATNTCCNCCACNCCCGCGNTCTACNTATGCCTNATNNTCGTGNCACTCCNGGNTTTCANAGGAACATCTGGGAGGCGGTAAGGTAGGGTTTGNTNGNAACNATNAATGGGGTTT\n",
            "  Warning: Sequence contained non-ACGT characters and was not added.\n",
            "\n",
            "✅ All valid sequence variants saved to: /content/generated_sequences_expr_7.04.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Updated Autoencoder Training with Transformer Encoder ===\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Load & Preprocess Data\n",
        "# ----------------------------\n",
        "base_to_idx = {'A': 0, 'T': 1, 'C': 2, 'G': 3, 'N': 4}\n",
        "idx_to_base = {v: k for k, v in base_to_idx.items()}\n",
        "\n",
        "def preprocess_sequence(seq, max_len=165):\n",
        "    seq = seq.upper()[:max_len]\n",
        "    seq += 'N' * (max_len - len(seq))\n",
        "    return [base_to_idx.get(base, 4) for base in seq]\n",
        "\n",
        "df1 = pd.read_csv('/content/ecoli_mpra_expr.csv')\n",
        "df2 = pd.read_csv('/content/ecoli_natural50bp_expr (1).csv')\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "combined_df['tokenized'] = combined_df['seq'].apply(preprocess_sequence)\n",
        "\n",
        "X_seq = np.array(combined_df['tokenized'].tolist())\n",
        "y_expr = combined_df['expr'].values.reshape(-1, 1)\n",
        "y_seq = np.expand_dims(X_seq, -1)\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Transformer Encoder Block\n",
        "# ----------------------------\n",
        "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.drop1 = layers.Dropout(rate)\n",
        "        self.drop2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        out1 = self.norm1(inputs + self.drop1(attn_output, training=training))\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.norm2(out1 + self.drop2(ffn_output, training=training))\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Define Model\n",
        "# ----------------------------\n",
        "vocab_size = 5\n",
        "max_len = 165\n",
        "embedding_dim = 16\n",
        "latent_dim = 64\n",
        "\n",
        "input_seq = layers.Input(shape=(max_len,), dtype='int32')\n",
        "x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_seq)\n",
        "x = TransformerEncoderBlock(embed_dim=embedding_dim, num_heads=4, ff_dim=latent_dim)(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "latent = layers.Dense(latent_dim, activation='relu', name=\"latent_vector\")(x)\n",
        "\n",
        "expr_pred = layers.Dense(1, name=\"expression_output\")(latent)\n",
        "\n",
        "x = layers.RepeatVector(max_len)(latent)\n",
        "x = layers.GRU(64, return_sequences=True)(x)\n",
        "decoded = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'), name=\"decoder_output\")(x)\n",
        "\n",
        "autoencoder = models.Model(inputs=input_seq, outputs=[decoded, expr_pred])\n",
        "autoencoder.compile(optimizer='adam',\n",
        "                    loss={'decoder_output': 'sparse_categorical_crossentropy', 'expression_output': 'mse'},\n",
        "                    loss_weights={'decoder_output': 1.0, 'expression_output': 1.0},\n",
        "                    metrics={'decoder_output': 'accuracy'})\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Train Model\n",
        "# ----------------------------\n",
        "autoencoder.fit(X_seq,\n",
        "                {'decoder_output': y_seq, 'expression_output': y_expr},\n",
        "                batch_size=64,\n",
        "                epochs=20,\n",
        "                validation_split=0.1)\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Save Model & Vocabulary\n",
        "# ----------------------------\n",
        "autoencoder.save(\"autoencoder_model.h5\")\n",
        "encoder = models.Model(autoencoder.input, autoencoder.get_layer(\"latent_vector\").output)\n",
        "\n",
        "latent_input = tf.keras.Input(shape=(latent_dim,))\n",
        "x = layers.RepeatVector(max_len)(latent_input)\n",
        "x = layers.GRU(64, return_sequences=True)(x)\n",
        "decoder_output = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))(x)\n",
        "decoder = tf.keras.Model(latent_input, decoder_output)\n",
        "decoder.save(\"decoder_model.h5\")\n",
        "\n",
        "expr_head = layers.Dense(1)(latent_input)\n",
        "expression_model = tf.keras.Model(latent_input, expr_head)\n",
        "expression_model.save(\"expression_model.h5\")\n",
        "\n",
        "import pickle\n",
        "vocab_mapping = {\"base_to_idx\": base_to_idx, \"idx_to_base\": idx_to_base}\n",
        "with open(\"vocab_mapping.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vocab_mapping, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf2-eM1FNJ4M",
        "outputId": "9ea969cc-d448-4eea-9034-34c505c92d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 46ms/step - decoder_output_accuracy: 0.4710 - decoder_output_loss: 1.1386 - expression_output_loss: 10.2037 - loss: 11.3423 - val_decoder_output_accuracy: 0.7755 - val_decoder_output_loss: 0.4464 - val_expression_output_loss: 4.7048 - val_loss: 5.1384\n",
            "Epoch 2/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5163 - decoder_output_loss: 0.9721 - expression_output_loss: 5.5762 - loss: 6.5483 - val_decoder_output_accuracy: 0.7665 - val_decoder_output_loss: 0.4731 - val_expression_output_loss: 4.5914 - val_loss: 5.0523\n",
            "Epoch 3/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5255 - decoder_output_loss: 0.9594 - expression_output_loss: 5.5322 - loss: 6.4916 - val_decoder_output_accuracy: 0.7797 - val_decoder_output_loss: 0.4326 - val_expression_output_loss: 4.5777 - val_loss: 4.9990\n",
            "Epoch 4/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5263 - decoder_output_loss: 0.9593 - expression_output_loss: 5.5664 - loss: 6.5258 - val_decoder_output_accuracy: 0.7842 - val_decoder_output_loss: 0.4283 - val_expression_output_loss: 4.6968 - val_loss: 5.1125\n",
            "Epoch 5/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5276 - decoder_output_loss: 0.9583 - expression_output_loss: 5.5669 - loss: 6.5252 - val_decoder_output_accuracy: 0.7816 - val_decoder_output_loss: 0.4228 - val_expression_output_loss: 4.5534 - val_loss: 4.9677\n",
            "Epoch 6/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - decoder_output_accuracy: 0.5257 - decoder_output_loss: 0.9640 - expression_output_loss: 5.5894 - loss: 6.5533 - val_decoder_output_accuracy: 0.7831 - val_decoder_output_loss: 0.4241 - val_expression_output_loss: 4.5593 - val_loss: 4.9746\n",
            "Epoch 7/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - decoder_output_accuracy: 0.5282 - decoder_output_loss: 0.9586 - expression_output_loss: 5.5030 - loss: 6.4616 - val_decoder_output_accuracy: 0.7837 - val_decoder_output_loss: 0.4231 - val_expression_output_loss: 4.6394 - val_loss: 5.0568\n",
            "Epoch 8/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5297 - decoder_output_loss: 0.9556 - expression_output_loss: 5.5023 - loss: 6.4578 - val_decoder_output_accuracy: 0.7815 - val_decoder_output_loss: 0.4262 - val_expression_output_loss: 4.5754 - val_loss: 4.9955\n",
            "Epoch 9/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5311 - decoder_output_loss: 0.9532 - expression_output_loss: 5.4459 - loss: 6.3992 - val_decoder_output_accuracy: 0.7870 - val_decoder_output_loss: 0.4164 - val_expression_output_loss: 4.5564 - val_loss: 4.9646\n",
            "Epoch 10/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5306 - decoder_output_loss: 0.9551 - expression_output_loss: 5.5898 - loss: 6.5449 - val_decoder_output_accuracy: 0.7914 - val_decoder_output_loss: 0.4145 - val_expression_output_loss: 4.5450 - val_loss: 4.9526\n",
            "Epoch 11/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5322 - decoder_output_loss: 0.9535 - expression_output_loss: 5.4549 - loss: 6.4083 - val_decoder_output_accuracy: 0.7920 - val_decoder_output_loss: 0.4126 - val_expression_output_loss: 4.6005 - val_loss: 5.0045\n",
            "Epoch 12/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - decoder_output_accuracy: 0.5288 - decoder_output_loss: 0.9594 - expression_output_loss: 5.6235 - loss: 6.5829 - val_decoder_output_accuracy: 0.7918 - val_decoder_output_loss: 0.4120 - val_expression_output_loss: 4.6432 - val_loss: 5.0512\n",
            "Epoch 13/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - decoder_output_accuracy: 0.5340 - decoder_output_loss: 0.9503 - expression_output_loss: 5.5001 - loss: 6.4504 - val_decoder_output_accuracy: 0.7843 - val_decoder_output_loss: 0.4269 - val_expression_output_loss: 4.5574 - val_loss: 4.9781\n",
            "Epoch 14/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5345 - decoder_output_loss: 0.9492 - expression_output_loss: 5.5240 - loss: 6.4731 - val_decoder_output_accuracy: 0.7903 - val_decoder_output_loss: 0.4131 - val_expression_output_loss: 4.5890 - val_loss: 4.9971\n",
            "Epoch 15/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5348 - decoder_output_loss: 0.9507 - expression_output_loss: 5.4815 - loss: 6.4322 - val_decoder_output_accuracy: 0.7892 - val_decoder_output_loss: 0.4115 - val_expression_output_loss: 4.5634 - val_loss: 4.9662\n",
            "Epoch 16/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5334 - decoder_output_loss: 0.9529 - expression_output_loss: 5.5035 - loss: 6.4564 - val_decoder_output_accuracy: 0.7934 - val_decoder_output_loss: 0.4107 - val_expression_output_loss: 4.5566 - val_loss: 4.9612\n",
            "Epoch 17/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5350 - decoder_output_loss: 0.9535 - expression_output_loss: 5.5160 - loss: 6.4695 - val_decoder_output_accuracy: 0.7948 - val_decoder_output_loss: 0.4171 - val_expression_output_loss: 4.7719 - val_loss: 5.1860\n",
            "Epoch 18/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5312 - decoder_output_loss: 0.9569 - expression_output_loss: 5.5794 - loss: 6.5364 - val_decoder_output_accuracy: 0.7947 - val_decoder_output_loss: 0.4162 - val_expression_output_loss: 4.6019 - val_loss: 5.0136\n",
            "Epoch 19/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5342 - decoder_output_loss: 0.9533 - expression_output_loss: 5.5117 - loss: 6.4650 - val_decoder_output_accuracy: 0.7953 - val_decoder_output_loss: 0.4104 - val_expression_output_loss: 4.6494 - val_loss: 5.0553\n",
            "Epoch 20/20\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 39ms/step - decoder_output_accuracy: 0.5350 - decoder_output_loss: 0.9526 - expression_output_loss: 5.5649 - loss: 6.5175 - val_decoder_output_accuracy: 0.7971 - val_decoder_output_loss: 0.4093 - val_expression_output_loss: 4.5768 - val_loss: 4.9812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import re\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# === Transformer Block for Loading ===\n",
        "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.drop1 = layers.Dropout(rate)\n",
        "        self.drop2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        out1 = self.norm1(inputs + self.drop1(attn_output, training=training))\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.norm2(out1 + self.drop2(ffn_output, training=training))\n",
        "\n",
        "# === Load Models and Vocab ===\n",
        "autoencoder = tf.keras.models.load_model(\n",
        "    \"autoencoder_model (1).h5\",\n",
        "    custom_objects={'TransformerEncoderBlock': TransformerEncoderBlock},\n",
        "    compile=False\n",
        ")\n",
        "decoder = tf.keras.models.load_model(\"decoder_model (1).h5\", compile=False)\n",
        "expression_model = tf.keras.models.load_model(\"expression_model (1).h5\", compile=False)\n",
        "\n",
        "with open(\"vocab_mapping (1).pkl\", \"rb\") as f:\n",
        "    vocab_data = pickle.load(f)\n",
        "idx_to_char = vocab_data[\"idx_to_base\"]\n",
        "\n",
        "# === Helper Functions ===\n",
        "def decode_sequence_with_temperature(logits, temperature=1.0):\n",
        "    probs = tf.nn.softmax(logits / temperature, axis=-1).numpy()\n",
        "    sampled_indices = [np.random.choice(len(p), p=p) for p in probs]\n",
        "    return ''.join([idx_to_char.get(i, 'N') for i in sampled_indices])\n",
        "\n",
        "def clean_dna_sequence(seq):\n",
        "    return re.sub(r'[^ACGT]', '', seq)\n",
        "\n",
        "# === Generation Function ===\n",
        "latent_dim = autoencoder.get_layer(\"latent_vector\").output.shape[-1]\n",
        "\n",
        "def run_promoter_generator(target_expr, steps=500, lr=0.05, temps=[0.4, 0.6, 0.8, 1.0]):\n",
        "    z = tf.Variable(tf.random.normal([1, latent_dim]), trainable=True)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    for step in range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred_expr = expression_model(z, training=False)\n",
        "            loss = tf.reduce_mean(tf.square(pred_expr - target_expr))\n",
        "        grads = tape.gradient(loss, [z])\n",
        "        optimizer.apply_gradients(zip(grads, [z]))\n",
        "\n",
        "    logits = decoder(z, training=False).numpy().squeeze()\n",
        "    results = []\n",
        "\n",
        "    for temp in temps:\n",
        "        seq = decode_sequence_with_temperature(logits, temperature=temp)\n",
        "        pred_expr = float(expression_model(z, training=False).numpy().squeeze())\n",
        "        results.append({\n",
        "    \"temperature\": temp,\n",
        "    \"predicted_expression\": round(pred_expr, 4),\n",
        "    \"sequence\": seq\n",
        "})\n",
        "\n",
        "\n",
        "    return results\n",
        "\n",
        "# === Example Usage ===\n",
        "if __name__ == \"__main__\":\n",
        "    target = float(input(\"Enter desired expression value: \"))\n",
        "    result = run_promoter_generator(target)\n",
        "    print(\"result:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl0RDTcbShq8",
        "outputId": "4ddcc21c-1ad6-4ba5-f8a7-f7300f13a9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter desired expression value: 80\n",
            "result: [{'temperature': 0.4, 'predicted_expression': 79.9556, 'sequence': 'ANCCAACACACCNCGGAAANTCCCACACTAAANGNTNGTATCTTNCACATGANCNNCATGTCAATCAAAAACAGCGCCGACCGAAACCCACACAAAGGAACCAGNCGAATCTCCGCAAACTCCTANTGCACCATCCTTCCCNACTGCGCCACNTANCGCTCACTA'}, {'temperature': 0.6, 'predicted_expression': 79.9556, 'sequence': 'GCATCGGCAATNANGANTAATNANCACTNTCAGGCGACCCTCTCGGTACGGGGCAACACCGTAATACAGGCCCCTCAACTACGTTAAGGATTTNATACGTTACCNGACGAAGANTACAATNTTTGTGTCGCTCGAAGTACANACACGACCCAACGCCTCTCAAAA'}, {'temperature': 0.8, 'predicted_expression': 79.9556, 'sequence': 'CATNCTCNNTCCCATCGCGTATNAGCTCCAAGTACCCCNNCNGCTAGNCGCATGCCANGGGANACNGCNTGNAGTGNGNGNCNCGNNAGNCNNACCACACCGCNTTAATACCCTGNAANNNNAACCAAANTGCTCTCCNTTTANACNTGCTTAACTTANCNNATG'}, {'temperature': 1.0, 'predicted_expression': 79.9556, 'sequence': 'NCNCCANTTAAAANANGNNGNAACTAAATTNCATAGCGGCNNNTNTATGCGNANGTCAGCACGCCGANTAAAGACAGCTTCNTTCGATANGAGCTGATTNNGAATCGTNANCGGANCAAACACAAGCNGTTCCNANTCCTNTNTNGCCTTATTCANANACATACC'}]\n"
          ]
        }
      ]
    }
  ]
}