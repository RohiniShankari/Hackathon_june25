{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhxmeeKW-Z8x",
        "outputId": "c2798372-ffc0-48fa-f4d3-021e9bfaa985",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.4.8)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.86.0)\n",
            "Requirement already satisfied: langchain-anthropic in /usr/local/lib/python3.11/dist-packages (0.3.15)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.38.37)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.65)\n",
            "Requirement already satisfied: langgraph-checkpoint>=2.0.26 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.1.0)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.2.2)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.70)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.5)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: anthropic<1,>=0.52.0 in /usr/local/lib/python3.11/dist-packages (from langchain-anthropic) (0.54.0)\n",
            "Requirement already satisfied: botocore<1.39.0,>=1.38.37 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.38.37)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.13.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.37->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.37->boto3) (2.4.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint>=2.0.26->langgraph) (1.10.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.37->boto3) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Required Packages\n",
        "!pip install langgraph langchain openai langchain-anthropic boto3 langchain_community\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.chat_models.base import BaseChatModel\n",
        "\n",
        "\n",
        "import boto3\n",
        "import json\n",
        "access_key=userdata.get('aws_access_key_id')\n",
        "secret_key=userdata.get('aws_secret_access_key')\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = access_key\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] =secret_key\n",
        "\n",
        "os.environ[\"AWS_REGION\"] = \"us-east-1\"  # or another valid region"
      ],
      "metadata": {
        "id": "JG11Cdko-eF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import BedrockChat as ChatBedrock\n",
        "\n"
      ],
      "metadata": {
        "id": "F7lusSPOCjoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claude = ChatBedrock(\n",
        "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # Claude v3 Sonnet on Bedrock\n",
        "    region_name=\"us-east-1\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "vIDeThsLCziu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import re\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# === Transformer Block for Custom Object ===\n",
        "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.drop1 = layers.Dropout(rate)\n",
        "        self.drop2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        out1 = self.norm1(inputs + self.drop1(attn_output, training=training))\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.norm2(out1 + self.drop2(ffn_output, training=training))\n",
        "\n",
        "# === Load Models ===\n",
        "autoencoder = tf.keras.models.load_model(\n",
        "    \"autoencoder_model.h5\",\n",
        "    custom_objects={'TransformerEncoderBlock': TransformerEncoderBlock},\n",
        "    compile=False\n",
        ")\n",
        "decoder = tf.keras.models.load_model(\"decoder_model.h5\", compile=False)\n",
        "expression_model = tf.keras.models.load_model(\"expression_model.h5\", compile=False)\n",
        "\n",
        "# === Load Vocabulary ===\n",
        "with open(\"vocab_mapping.pkl\", \"rb\") as f:\n",
        "    vocab_data = pickle.load(f)\n",
        "vocab = vocab_data[\"base_to_idx\"]\n",
        "idx_to_char = vocab_data[\"idx_to_base\"]\n",
        "\n",
        "# === Sequence Decoding Functions ===\n",
        "def decode_sequence_with_temperature(logits, temperature=1.0):\n",
        "    probs = tf.nn.softmax(logits / temperature, axis=-1).numpy()\n",
        "    sampled_indices = [np.random.choice(len(p), p=p) for p in probs]\n",
        "    chars = [idx_to_char.get(i, 'N') for i in sampled_indices]\n",
        "    return ''.join(chars)\n",
        "\n",
        "def clean_dna_sequence(seq):\n",
        "    return re.sub(r'[^ACGT]', '', seq)\n",
        "\n",
        "# === Promoter Generation Function ===\n",
        "latent_dim = autoencoder.get_layer(\"latent_vector\").output.shape[-1]\n",
        "\n",
        "def generate_promoter_sequences(target_expr, steps=500, lr=0.05, temps=[0.4, 0.6, 0.8, 1.0]):\n",
        "    z = tf.Variable(tf.random.normal([1, latent_dim]), trainable=True)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    for _ in range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred_expr = expression_model(z, training=False)\n",
        "            loss = tf.reduce_mean(tf.square(pred_expr - target_expr))\n",
        "        grads = tape.gradient(loss, [z])\n",
        "        optimizer.apply_gradients(zip(grads, [z]))\n",
        "\n",
        "    logits = decoder(z, training=False).numpy().squeeze()\n",
        "\n",
        "    generated_variants = []\n",
        "    for temp in temps:\n",
        "        seq = decode_sequence_with_temperature(logits, temperature=temp)\n",
        "        cleaned = clean_dna_sequence(seq)\n",
        "        if len(cleaned) == len(seq):\n",
        "            pred_expr = expression_model(z, training=False).numpy().squeeze()\n",
        "            generated_variants.append((seq, float(pred_expr)))\n",
        "\n",
        "    return generated_variants\n"
      ],
      "metadata": {
        "id": "__Ch6dLkUCWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import re\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# === Transformer Block for Loading ===\n",
        "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.drop1 = layers.Dropout(rate)\n",
        "        self.drop2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        out1 = self.norm1(inputs + self.drop1(attn_output, training=training))\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.norm2(out1 + self.drop2(ffn_output, training=training))\n",
        "\n",
        "# === Load Models and Vocab ===\n",
        "autoencoder = tf.keras.models.load_model(\n",
        "    \"autoencoder_model.h5\",\n",
        "    custom_objects={'TransformerEncoderBlock': TransformerEncoderBlock},\n",
        "    compile=False\n",
        ")\n",
        "decoder = tf.keras.models.load_model(\"decoder_model.h5\", compile=False)\n",
        "expression_model = tf.keras.models.load_model(\"expression_model.h5\", compile=False)\n",
        "\n",
        "with open(\"vocab_mapping.pkl\", \"rb\") as f:\n",
        "    vocab_data = pickle.load(f)\n",
        "idx_to_char = vocab_data[\"idx_to_base\"]\n",
        "\n",
        "# === Helper Functions ===\n",
        "def decode_sequence_with_temperature(logits, temperature=1.0):\n",
        "    probs = tf.nn.softmax(logits / temperature, axis=-1).numpy()\n",
        "    sampled_indices = [np.random.choice(len(p), p=p) for p in probs]\n",
        "    return ''.join([idx_to_char.get(i, 'N') for i in sampled_indices])\n",
        "\n",
        "def clean_dna_sequence(seq):\n",
        "    return re.sub(r'[^ACGT]', '', seq)\n",
        "\n",
        "# === Generation Function ===\n",
        "latent_dim = autoencoder.get_layer(\"latent_vector\").output.shape[-1]\n",
        "\n",
        "def run_promoter_generator(target_expr, steps=500, lr=0.05, temps=[0.4, 0.6, 0.8, 1.0]):\n",
        "    z = tf.Variable(tf.random.normal([1, latent_dim]), trainable=True)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    for step in range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred_expr = expression_model(z, training=False)\n",
        "            loss = tf.reduce_mean(tf.square(pred_expr - target_expr))\n",
        "        grads = tape.gradient(loss, [z])\n",
        "        optimizer.apply_gradients(zip(grads, [z]))\n",
        "\n",
        "    logits = decoder(z, training=False).numpy().squeeze()\n",
        "    results = []\n",
        "\n",
        "    for temp in temps:\n",
        "        seq = decode_sequence_with_temperature(logits, temperature=temp)\n",
        "        pred_expr = float(expression_model(z, training=False).numpy().squeeze())\n",
        "        results.append({\n",
        "    \"temperature\": temp,\n",
        "    \"predicted_expression\": round(pred_expr, 4),\n",
        "    \"sequence\": seq\n",
        "})\n",
        "\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hl0RDTcbShq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = run_promoter_generator(70)\n",
        "print(\"result:\", result)\n",
        "print(result[1][\"sequence\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKhSKpvJurUA",
        "outputId": "7e89c6fb-1c3a-4d5a-9787-268a122f99f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result: [{'temperature': 0.4, 'predicted_expression': 69.9947, 'sequence': 'ATCTTTCCACCCCACNAGTCNNGTNCGAAANCCACCGCCNACACTTGACCCCAACCCGGTAANNGCCNACGTCAACCACCAAAACAGCCNACACACCCATCANCATGCCTGNNAAAAACCACCACCATNACCCAAACCCGAANNCAANNANTNNCAATNTAAAAA'}, {'temperature': 0.6, 'predicted_expression': 69.9947, 'sequence': 'CNACGTNCGGAGCNCCNGCACCCNGCCCTTAAGATGCCNCCCAATNCANCACANACTANGCACTGGNNCNCCNTTCNACNNTCNCCNGACTNCACCGTGCTCNAAGCNCCACCATNCAANACCTCTCCGCCCTACGGGNCCGNCATANNNCNAGACANCCGGCNA'}, {'temperature': 0.8, 'predicted_expression': 69.9947, 'sequence': 'TCAATATGNTTCACCANNCACTCCTAAGCCATCATTGCCATAAGGNGCGNNGATCTGCTCATCCNAGCAAATCAACCNAGAAANAACCNAAGCNANACNAAATTACTACNNATCNNGGTNNGATAGAGNACANATGGCCAGCCCATGGCTGCCCTCAANAGTTNG'}, {'temperature': 1.0, 'predicted_expression': 69.9947, 'sequence': 'CTNACNTCANCGCCGCTCCCGCCNNCGNGTCGCGCGTGAATATACGACGNAANNANNTCTTGGGACATTACNGCCNTNCNCCGNAGCCGACCNAATTACTACTTTCCCGTAAACTCNAGNNCTGNCCGCANGAAAGCNTCCCTTNGACGNATGAGCGNTGTAAAT'}]\n",
            "CNACGTNCGGAGCNCCNGCACCCNGCCCTTAAGATGCCNCCCAATNCANCACANACTANGCACTGGNNCNCCNTTCNACNNTCNCCNGACTNCACCGTGCTCNAAGCNCCACCATNCAANACCTCTCCGCCCTACGGGNCCGNCATANNNCNAGACANCCGGCNA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Imports ------------------\n",
        "import os\n",
        "from typing import TypedDict\n",
        "from langchain_community.chat_models import BedrockChat as ChatBedrock\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# ------------------ AWS Credential Setup ------------------\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "# Replace with your actual credential logic\n",
        "access_key = userdata.get('aws_access_key_id')\n",
        "secret_key = userdata.get('aws_secret_access_key')\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = access_key\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = secret_key\n",
        "os.environ[\"AWS_REGION\"] = \"us-east-1\"\n",
        "\n",
        "# ------------------ Claude via Bedrock ------------------\n",
        "\n",
        "claude = ChatBedrock(\n",
        "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "    region_name=\"us-east-1\"\n",
        ")\n",
        "\n",
        "# ------------------ State Schema ------------------\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    user_query: str\n",
        "    target_expression: float\n",
        "    promoter: str\n",
        "    wet_lab: str\n",
        "    literature: str\n",
        "    report: str\n",
        "\n",
        "# ------------------ LangGraph Nodes ------------------\n",
        "\n",
        "# âœ¨ Node 1: Extract expression value from query\n",
        "def extract_expression_node(state: GraphState) -> dict:\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Extract the numeric expression value from this query: {user_query}. Only return the number.\"\n",
        "    )\n",
        "    chain = prompt | claude | (lambda x: {\"target_expression\": float(x.content.strip())})\n",
        "    return chain.invoke({\"user_query\": state[\"user_query\"]})\n",
        "\n",
        "\n",
        "# âœ¨ Node 2: Use your own model to generate promoter\n",
        "def generate_promoter_node(state: GraphState) -> dict:\n",
        "    expression = state[\"target_expression\"]  # dynamically get from state\n",
        "    result = run_promoter_generator(expression)  # your custom function\n",
        "\n",
        "    promoter_seq = result[1][\"sequence\"]  # assuming result is a tuple or list and this works\n",
        "\n",
        "    return {\"promoter\": promoter_seq}\n",
        "\n",
        "\n",
        "# âœ¨ Node 3: Suggest wet lab experiment\n",
        "def suggest_wet_lab_node(state: GraphState) -> dict:\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"You are a molecular biologist designing an experiment to validate the function of a synthetic promoter in *E. coli*.\n",
        "\n",
        "The user has requested the following:\n",
        "\"{query}\"\n",
        "\n",
        "The promoter sequence is: {promoter}\n",
        "\n",
        "Design a wet lab experiment to test its activity. Include:\n",
        "- The experimental method (e.g., reporter assay, transformation protocol)\n",
        "- The plasmid or vector system (if relevant)\n",
        "- The host strain of *E. coli* to use\n",
        "- What will be measured (e.g., fluorescence, enzymatic activity, mRNA level)\n",
        "- Controls to include for comparison\n",
        "\n",
        "Incorporate any specific requirements mentioned by the user.\n",
        "Provide a clear and concise protocol outline.\"\"\"\n",
        "    )\n",
        "    chain = prompt | claude | (lambda x: {\"wet_lab\": x.content.strip()})\n",
        "    return chain.invoke({\n",
        "        \"query\": state[\"user_query\"],\n",
        "        \"promoter\": state[\"promoter\"]\n",
        "    })\n",
        "\n",
        "# âœ¨ Node 4: Summarize research literature\n",
        "def summarize_research_node(state: GraphState) -> dict:\n",
        "    user_query = f\"\"\"\n",
        "The user is interested in this query: \"{state['user_query']}\"\n",
        "\n",
        "Find and summarize recent research related to the synthetic promoter sequence:\n",
        "\"{state['promoter']}\" in *E. coli*.\n",
        "\n",
        "Focus on studies in:\n",
        "- Synthetic biology\n",
        "- Gene expression tuning\n",
        "- Genetic circuit design\n",
        "\n",
        "Especially highlight anything that aligns with the user's request.\n",
        "\n",
        "Provide:\n",
        "- A concise summary of relevant findings\n",
        "- Bullet points for important insights (if applicable)\n",
        "- Direct links to peer-reviewed publications, preprints (e.g., PubMed, bioRxiv, Nature, NAR, etc.)\n",
        "- Mention if the exact promoter has been characterized or modified\n",
        "\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"You are a scientific research assistant helping summarize the literature.\\n\\n\"\n",
        "        \"{query}\\n\\n\"\n",
        "        \"Return your answer with:\\n\"\n",
        "        \"- A brief summary of the key research findings\\n\"\n",
        "        \"- Bullet points for important insights (if applicable)\\n\"\n",
        "        \"- Direct links to relevant publications (if available)\\n\"\n",
        "        \"- Focus on *E. coli* and synthetic promoter usage\"\n",
        "    )\n",
        "    chain = prompt | claude | (lambda x: {\"literature\": x.content.strip()})\n",
        "    return chain.invoke({\"query\": user_query})\n",
        "\n",
        "def assemble_report_node(state: GraphState) -> dict:\n",
        "    # Step 1: Build the full report\n",
        "    full_report = f\"\"\"\n",
        "ðŸ§¬ **Synthetic Promoter Generation Report**\n",
        "\n",
        "ðŸ”¹ **Target Expression:** {state['target_expression']}\n",
        "\n",
        "ðŸ§¬ **Generated Promoter Sequence:**\n",
        "{state['promoter']}\n",
        "\n",
        "ðŸ§ª **Wet Lab Experiment Suggestion:**\n",
        "{state['wet_lab']}\n",
        "\n",
        "ðŸ“š **Research Summary:**\n",
        "{state['literature']}\n",
        "\"\"\"\n",
        "\n",
        "    # Step 2: Summarize the report using Claude\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"You are a synthetic biology expert reviewing this project report.\n",
        "\n",
        "Please do the following:\n",
        "- Carefully read the report.\n",
        "- Identify and correct any factual, scientific, or formatting errors.\n",
        "- Think critically about the experimental design and relevance of the research.\n",
        "- Improve clarity where needed.\n",
        "- Return a revised version that is suitable for inclusion in a scientific update or proposal.\"\"\"\n",
        "\n",
        "    )\n",
        "    chain = prompt | claude | (lambda x: {\"summary\": x.content.strip()})\n",
        "    result = chain.invoke({\"report\": full_report})\n",
        "\n",
        "    # Step 3: Return both the report and the summary\n",
        "    return {\n",
        "        \"report\": full_report.strip(),\n",
        "        \"summary\": result[\"summary\"]\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------ LangGraph Definition ------------------\n",
        "\n",
        "graph = StateGraph(GraphState)\n",
        "\n",
        "graph.add_node(\"extract\", extract_expression_node)\n",
        "graph.add_node(\"generate\", generate_promoter_node)\n",
        "graph.add_node(\"experiment\", suggest_wet_lab_node)\n",
        "graph.add_node(\"summarize\", summarize_research_node)\n",
        "graph.add_node(\"assemble_report\", assemble_report_node)\n",
        "\n",
        "graph.set_entry_point(\"extract\")\n",
        "graph.add_edge(\"extract\", \"generate\")\n",
        "graph.add_edge(\"generate\", \"experiment\")\n",
        "graph.add_edge(\"experiment\", \"summarize\")\n",
        "graph.add_edge(\"summarize\", \"assemble_report\")\n",
        "graph.set_finish_point(\"assemble_report\")\n",
        "\n",
        "workflow = graph.compile()\n",
        "\n",
        "# ------------------ Run the Agent ------------------\n",
        "\n",
        "user_query = \"Develop a synthetic promoter with 40 expression value\"\n",
        "\n",
        "result = workflow.invoke({\"user_query\": user_query})\n",
        "\n",
        "print(result[\"report\"])\n"
      ],
      "metadata": {
        "id": "ZwriFX26wErQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "068d6357-eaa7-4ed4-8273-1666a90dd3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§¬ **Synthetic Promoter Generation Report**\n",
            "\n",
            "ðŸ”¹ **Target Expression:** 40.0\n",
            "\n",
            "ðŸ§¬ **Generated Promoter Sequence:**\n",
            "NACCNCCCATCAANCGCNNATCTGGGCAAATAGATCNNCACCGGTCAAGCNGNTGAANCANCAGGACAACACNTAAAANCNNNCNAGGAANCGTTANCAAGTCCCGCNANTCGCCCATACTNNCAGAACATGTTTATCACAGANCNTAACNCCAAAANCGGNGAA\n",
            "\n",
            "ðŸ§ª **Wet Lab Experiment Suggestion:**\n",
            "To validate the function of the synthetic promoter sequence with the requested expression value of 40, we can design a reporter assay using a fluorescent protein as the reporter gene. Here is an outline of the experimental protocol:\n",
            "\n",
            "Experimental Method: Fluorescent Protein Reporter Assay\n",
            "\n",
            "Plasmid/Vector System:\n",
            "- Use a low-copy plasmid vector compatible with E. coli, such as pBR322 or pUC19.\n",
            "- Clone the synthetic promoter sequence upstream of a fluorescent reporter gene, such as GFP or RFP.\n",
            "\n",
            "Host Strain:\n",
            "- Use a common E. coli strain suitable for cloning and expression studies, such as DH5Î± or BL21(DE3).\n",
            "\n",
            "Measurement:\n",
            "- Measure the fluorescence intensity of the reporter protein, which will be proportional to the promoter activity.\n",
            "\n",
            "Controls:\n",
            "1. Negative Control:\n",
            "   - Use the same plasmid vector without the synthetic promoter, or with a non-functional promoter sequence.\n",
            "   - This control will establish the baseline fluorescence level.\n",
            "\n",
            "2. Positive Control:\n",
            "   - Use a well-characterized strong promoter, such as the lac promoter or T7 promoter, driving the expression of the same fluorescent reporter gene.\n",
            "   - This control will serve as a reference for high promoter activity.\n",
            "\n",
            "3. Additional Controls (Optional):\n",
            "   - Include promoters with known expression levels (e.g., low, medium, high) to establish a calibration curve for comparing the synthetic promoter activity.\n",
            "\n",
            "Protocol Outline:\n",
            "\n",
            "1. Clone the synthetic promoter sequence upstream of the fluorescent reporter gene in the chosen plasmid vector.\n",
            "2. Transform the recombinant plasmid into the E. coli host strain using a standard transformation protocol (e.g., heat shock or electroporation).\n",
            "3. Inoculate transformed cells into liquid culture medium with appropriate antibiotics for plasmid selection.\n",
            "4. Incubate the cultures at optimal growth conditions (e.g., 37Â°C with shaking) until reaching mid-log phase.\n",
            "5. Measure the fluorescence intensity of the cultures using a fluorometer or a microplate reader with appropriate excitation and emission wavelengths for the chosen reporter.\n",
            "6. Normalize the fluorescence measurements by cell density (e.g., OD600) to account for differences in growth rates.\n",
            "7. Compare the normalized fluorescence values of the synthetic promoter construct with the negative and positive controls, as well as any additional calibration controls.\n",
            "8. Analyze the data to determine if the synthetic promoter exhibits the desired expression value of 40 relative to the controls.\n",
            "\n",
            "Note: If the user has specific requirements or preferences for the plasmid vector, host strain, or experimental conditions, adjust the protocol accordingly. Additionally, consider performing biological replicates and statistical analysis to ensure the reliability and reproducibility of the results.\n",
            "\n",
            "ðŸ“š **Research Summary:**\n",
            "Summary of Key Research Findings:\n",
            "\n",
            "The provided synthetic promoter sequence appears to be a novel, artificially designed sequence, as I did not find any direct studies characterizing its expression in Escherichia coli. However, there is relevant research on developing synthetic promoters, tuning gene expression, and genetic circuit design in E. coli that could provide insights into characterizing and optimizing this sequence.\n",
            "\n",
            "Important Insights:\n",
            "\n",
            "- Synthetic promoters can be designed and tuned by modifying regulatory elements, such as the -35 and -10 boxes, UP elements, and transcription factor binding sites, to achieve desired expression levels (Mutalik et al., 2013; Kosuri et al., 2013).\n",
            "- Computational models and libraries of well-characterized promoter parts can aid in the design and prediction of synthetic promoter strengths (Bao et al., 2018; Urtecho et al., 2019).\n",
            "- Directed evolution techniques, such as error-prone PCR and transcription factor engineering, can be used to optimize synthetic promoters for improved expression or dynamic regulation (Brophy and Voigt, 2014; Luo et al., 2021).\n",
            "- Synthetic promoters can be integrated into genetic circuits for various applications, such as metabolic engineering, biosensing, and synthetic gene networks (Blazeck and Alper, 2013; Meng et al., 2020).\n",
            "\n",
            "Relevant Publications:\n",
            "\n",
            "1. Mutalik, V. K., Guimaraes, J. C., Cambray, G., Lam, C., Christoffersen, M. J., Mai, Q. A., ... & Arkin, A. P. (2013). Precise and reliable gene expression via standard transcription and translation initiation elements. Nature Methods, 10(4), 354-360. https://doi.org/10.1038/nmeth.2404\n",
            "\n",
            "2. Kosuri, S., Goodman, D. B., Cambray, G., Mutalik, V. K., Gao, Y., Arkin, A. P., ... & Church, G. M. (2013). Composability of regulatory sequences controlling transcription and translation in Escherichia coli. Proceedings of the National Academy of Sciences, 110(34), 14024-14029. https://doi.org/10.1073/pnas.1301301110\n",
            "\n",
            "3. Bao, Z., Mudiraj, S., Torres-Delgado, D., & Vocadlo, D. J. (2018). A synthetic quantitative bacterial promoter device enabled by ligand-driven promoter binding. Nature Communications, 9(1), 1-10. https://doi.org/10.1038/s41467-018-06771-1\n",
            "\n",
            "4. Urtecho, G., Tripp, A. D., Insigne, K. D., Piasecki, S. K., & Smolke, C. D. (2019). Systematic construction of vectors for inducible or constitutive gene expression in Saccharomyces cerevisiae. Biotechnology and Bioengineering, 116(4), 905-920. https://doi.org/10.1002/bit.26923\n",
            "\n",
            "5. Brophy, J. A., & Voigt, C. A. (2014). Principles of genetic circuit design. Nature Methods, 11(5), 508-520. https://doi.org/10.1038/nmeth.2926\n",
            "\n",
            "6. Luo, B., Zhang, W., Wang, Z., Liu, Y., Zhao, X., Yu, H., & Lou, C. (2021). Directed evolution of synthetic promoter for high-level constitutive gene expression in Escherichia coli. Microbial Cell Factories, 20(1), 1-12. https://doi.org/10.1186/s12934-021-01532-5\n",
            "\n",
            "7. Blazeck, J., & Alper, H. S. (2013). Promoter engineering: Recent advances in controlling transcription at the most fundamental level. Biotechnology Journal, 8(1), 46-58. https://doi.org/10.1002/biot.201200120\n",
            "\n",
            "8. Meng, F., Dai, W., Zhen, Y., Guan, X., & Ma, Y. (2020). Design of synthetic promoters for tuning metabolic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def run_agent(user_query):\n",
        "    try:\n",
        "        result = workflow.invoke({\"user_query\": user_query})\n",
        "        report = result.get(\"report\", \"No report generated.\")\n",
        "\n",
        "        # Create a temp text file\n",
        "        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".txt\") as tmp_txt:\n",
        "            tmp_txt.write(report)\n",
        "            txt_path = tmp_txt.name\n",
        "\n",
        "        # Zip the text file to force download\n",
        "        zip_path = txt_path.replace(\".txt\", \".zip\")\n",
        "        with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "            zipf.write(txt_path, arcname=\"synthetic_promoter_report.txt\")\n",
        "\n",
        "        return report, zip_path\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\", None\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=run_agent,\n",
        "    inputs=gr.Textbox(label=\"Enter your promoter query or just enter the expression value\", placeholder=\"e.g., Develop a synthetic promoter with 40 expression value\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Agent Report\"),\n",
        "        gr.File(label=\"Download Report (ZIP)\")\n",
        "    ],\n",
        "    title=\"Synthetic Promoter Agent\",\n",
        "    description=\"This agent designs synthetic promoters and provides a report using a multi-step LLM pipeline.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "_1vxr-9kWgWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf33dc04-1473-40c7-ab62-c7b58e85744f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b9f2256e8096cfa512.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b9f2256e8096cfa512.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HiqhatSQ_6zp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}